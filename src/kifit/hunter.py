import logging
from typing import List
from itertools import (
    product,
    combinations
)

import numpy as np

from scipy.linalg import cho_factor, cho_solve
from scipy.odr import ODR, Model, RealData
from scipy.stats import chi2, linregress, multivariate_normal

from scipy.optimize import (
    minimize,
    dual_annealing,
    differential_evolution,
)


logging.basicConfig(level=logging.INFO)

np.random.seed(1)


def linfit(p, x):
    return p[0] * x + p[1]


def linfit_x(p, y):
    return (y - p[1]) / p[0]


def blocking_bounds(
        messenger,
        lbs: List[float],
        ubs: List[float]):
    """
    Blocking method to compute the statistical uncertainty of the confidence
    intervals.

    Args:
        lbs: list of lower bounds for the different experiments.
        ubs: list of upper bounds for the different experiments.
        block_size: number of points in each block used by the blocking method.
        Reduced to the number of surviving confidence intervals generated by the
        experiments if block_size is larger. However, in this case the
        uncertainties on the bounds cannot be computed.

    """

    if len(lbs) != len(ubs):
        raise ValueError("Lists of lower and upper bounds passed to the"
        "blocking method should be of equal length."
            f"len(lbs)={len(lbs)}, len(ubs)={len(ubs)}")

    block_size = messenger.runparams.block_size

    if block_size > len(lbs):
        block_size = len(lbs)
        print(f"Reducing block size to {len(lbs)} since not enough samples "
            "have been generated. \n"
            "sig[LB] and sig[UB] cannot be computed in this case.")

    nblocks = int(len(lbs) / block_size)
    print("nblocks", nblocks)

    # parametric bootstrap
    lb_min, ub_max, lb_val, ub_val, sig_lb, sig_ub = [], [], [], [], [], []

    for b in range(nblocks):
        lb_block = lbs[b * block_size: (b + 1) * block_size]
        ub_block = ubs[b * block_size: (b + 1) * block_size]

        lb_min.append(np.min(lb_block))
        ub_max.append(np.max(ub_block))

        lb_val.append(np.mean(lb_min))
        ub_val.append(np.mean(ub_max))

        sig_lb.append(np.std(lb_min))
        sig_ub.append(np.std(ub_max))

    print("sig_lb", sig_lb)
    print("sig_ub", sig_ub)

    sig_LB = sig_lb[-1]
    sig_UB = sig_ub[-1]

    LB = lb_val[-1] - sig_LB
    UB = ub_val[-1] + sig_UB

    if messenger.runparams.verbose is True and block_size < len(lbs):

        from kifit.artist import blocking_plot

        blocking_plot(
            messenger,
            nblocks=nblocks,
            estimations=lb_val,
            uncertainties=sig_lb,
            label="Lower bound",
            plotname="blocking_lb"
        )
        blocking_plot(
            messenger,
            nblocks=nblocks,
            estimations=ub_val,
            uncertainties=sig_ub,
            label="Upper bound",
            plotname="blocking_ub"
        )
    return LB, UB, sig_LB, sig_UB


def get_odr_residuals(p, x, y, sx, sy):

    v = 1 / np.sqrt(1 + p[0] ** 2) * np.array([-p[0], 1])
    z = np.array([x, y]).T
    sz = np.array([np.diag([sx[i] ** 2, sy[i] ** 2]) for i in range(len(x))])

    residuals = np.array([v @ z[i] - p[1] * v[1] for i in range(len(x))])
    sigresiduals = np.sqrt(np.array([v @ sz[i] @ v for i in range(len(x))]))

    return residuals, sigresiduals


def perform_linreg(isotopeshiftdata, reftrans_index: int = 0):
    """
    Perform linear regression.

    Args:
        data (normalised isotope shifts: rows=isotope pairs, columns=trans.)
        reference_transition_index (default: first transition)

    Returns:
        slopes, intercepts, Kperp, phi

    """

    x = isotopeshiftdata.T[reftrans_index]
    y = np.delete(isotopeshiftdata, reftrans_index, axis=1)

    betas = []
    sig_betas = []

    for i in range(y.shape[1]):
        res = linregress(x, y.T[i])
        betas.append([res.slope, res.intercept])
        sig_betas.append([res.stderr, res.intercept_stderr])

    betas = np.array(betas)
    sig_betas = np.array(sig_betas)

    ph1s = np.arctan(betas.T[0])
    sig_ph1s = np.array(
        [sig_betas[j, 0] / (1 + betas[j, 0]) for j in range(len(betas))]
    )

    kperp1s = betas.T[1] * np.cos(ph1s)
    sig_kperp1s = np.sqrt(
        (sig_betas.T[1] * np.cos(ph1s)) ** 2
        + (betas.T[1] * sig_ph1s * np.sin(ph1s)) ** 2
    )

    return (betas, sig_betas, kperp1s, ph1s, sig_kperp1s, sig_ph1s)


def perform_odr(isotopeshiftdata, sigisotopeshiftdata, reftrans_index: int = 0):
    """
    Perform separate orthogonal distance regression for each transition pair.

    Args:
        data (normalised isotope shifts: rows=isotope pairs, columns=trans.)
        reftrans_index (default: first transition)

    Returns:
        slopes, intercepts, kperp1, ph1, sig_kperp1, sig_ph1

    """
    lin_model = Model(linfit)

    x = isotopeshiftdata.T[reftrans_index]
    y = np.delete(isotopeshiftdata, reftrans_index, axis=1)

    sigx = sigisotopeshiftdata.T[reftrans_index]
    sigy = np.delete(sigisotopeshiftdata, reftrans_index, axis=1)

    betas = []
    sig_betas = []

    for i in range(y.shape[1]):
        data = RealData(x, y.T[i], sx=sigx, sy=sigy.T[i])
        beta_init = np.polyfit(x, y.T[i], 1)
        odr = ODR(data, lin_model, beta0=beta_init)
        out = odr.run()
        betas.append(out.beta)
        sig_betas.append(out.sd_beta)

    betas = np.array(betas)
    sig_betas = np.array(sig_betas)

    ph1s = np.arctan(betas.T[0])
    sig_ph1s = np.arctan(sig_betas.T[0])

    kperp1s = betas.T[1] * np.cos(ph1s)
    sig_kperp1s = np.sqrt(
        (sig_betas.T[1] * np.cos(ph1s)) ** 2
        + (betas.T[1] * sig_ph1s * np.sin(ph1s)) ** 2
    )

    return (betas, sig_betas, kperp1s, ph1s, sig_kperp1s, sig_ph1s)


def get_paramsamples(means, stdevs, nsamples):
    """
    Get nsamples samples of the parameters described by means and stdevs.

    """
    return multivariate_normal.rvs(means, np.diag(stdevs**2), size=nsamples)


def choLL(absd, covmat, lam=0):
    """
    For a given sample of absd, with the covariance matrix covmat, compute the
    log-likelihood using the Cholesky decomposition of covmat.

    """
    chol_covmat, lower = cho_factor(covmat + lam * np.eye(covmat.shape[0]), lower=True)

    absd_covinv_absd = absd.dot(cho_solve((chol_covmat, lower), absd))

    logdet = 2 * np.sum(np.log(np.diag(chol_covmat)))

    return 0.5 * (logdet + absd_covinv_absd)


def spectraLL(absd, covmat, lam=0):
    """
    For a given sample of absd, with the covariance matrix covmat, compute the
    log-likelihood using the spectral decomposition of covmat.
    """
    covmat += lam * np.eye(covmat.shape[0])
    eigenvalues, eigenvectors = np.linalg.eigh(covmat)

    inv_eigenvalues = 1.0 / eigenvalues
    covinv = eigenvectors @ np.diag(inv_eigenvalues) @ eigenvectors.T

    absd_covinv_absd = absd.dot(covinv).dot(absd)

    logdet = np.sum(np.log(eigenvalues))

    return 0.5 * (logdet + absd_covinv_absd)


def get_llist(absdsamples, nelemsamples, cov_decomp_method="cholesky"):
    """
    For a fixed alphaNP value, get ll for the nelemsamples samples of the input
    parameters.

    """
    # compute covariance matrix for fixed alpha value
    cov_absd = np.cov(np.array(absdsamples), rowvar=False)

    if cov_decomp_method == "cholesky":
        LL = choLL
    elif cov_decomp_method == "spectral":
        LL = spectraLL

    llist = []
    for s in range(nelemsamples):
        llist.append(LL(absdsamples[s], cov_absd))

    return np.array(llist)


def generate_element_sample(elem, nsamples: int):
    """
    Generate ``nsamples`` of ``elem`` varying the input parameters according
    to the provided standard deviations.
    """
    parameter_samples = get_paramsamples(
        elem.means_input_params, elem.stdevs_input_params, nsamples
    )
    return parameter_samples


def generate_alphaNP_sample(elem, nsamples: int, search_mode: str = "random",
        lb: float = None, ub: float = None):
    """
    Generate ``nsamples`` of alphaNP according to the initial conditions
    provided by the ``elem`` data. The sample can be generated either randomly
    or by use of a grid.

    """
    if search_mode == "random":
        alphaNP_samples = np.random.normal(
            elem.alphaNP_init, elem.sig_alphaNP_init, nsamples
        )
    elif search_mode == "grid":
        if lb is None or ub is None:
            alphaNP_samples = np.linspace(
                elem.alphaNP_init - elem.sig_alphaNP_init,
                elem.alphaNP_init + elem.sig_alphaNP_init,
                nsamples
            )
        else:
            alphaNP_samples = np.linspace(
                lb,
                ub,
                nsamples
            )

    return np.sort(alphaNP_samples)


def get_bestalphaNP_and_bounds(
        messenger,
        bestalphaNPlist,
        confints):
    """
    Starting from the best alphaNP values, apply the blocking method to
    compute the best alphaNP value, its uncertainty, as well as the nsigma -
    upper and lower bounds on alphaNP.

    """
    confints = np.array(confints)

    best_alpha = np.median(bestalphaNPlist)
    sig_alpha = np.std(bestalphaNPlist)

    lowerbounds_exps = confints.T[0]
    lb_nans = np.argwhere(np.isnan(lowerbounds_exps))
    upperbounds_exps = confints.T[1]
    ub_nans = np.argwhere(np.isnan(upperbounds_exps))

    np.alltrue(lb_nans == ub_nans)

    lowerbounds_exps = lowerbounds_exps[~np.isnan(lowerbounds_exps)]
    upperbounds_exps = upperbounds_exps[~np.isnan(upperbounds_exps)]

    if len(lowerbounds_exps) == 0 or len(upperbounds_exps) == 0:
        return (best_alpha, sig_alpha, np.nan, np.nan, np.nan, np.nan)

    print("lowbounds_exps", lowerbounds_exps)
    print("upperbounds_exps", upperbounds_exps)

    LB, UB, sig_LB, sig_UB = blocking_bounds(
        messenger,
        lowerbounds_exps,
        upperbounds_exps)

    print(f"Final result: {best_alpha} with bounds [{LB}, {UB}].")

    return (best_alpha, sig_alpha, LB, sig_LB, UB, sig_UB)


def compute_ll_experiments(
        elem_collection,
        alphasamples,
        nelemsamples,
        min_percentile,
        cov_decomp_method="cholesky"):
    """
    Generate alphaNP list for element ``elem`` according to ``parameters_samples``.

    Args:
        elem (Elem): target element.
        nsamples (int): number of samples.
        mphivar (bool): if ``True``, this procedure is repeated for all
            X-coefficients provided for elem.
        save_sample (bool): if ``True``, the parameters and alphaNP samples are saved.

    Return:
        List[float], List[float]: alphaNP samples and list of associated log likelihood.
    """

    elemsamples_collection = []
    for elem in elem_collection.elems:
        elemsamples_collection.append(generate_element_sample(elem, nelemsamples))

    lls = []

    # for each alpha in the list of generated alphas
    for s in range(len(alphasamples)):
        # we collect a list of absd for each generated combination alpha-fitparams
        lls.append(logL_alphaNP(
            alphaNP=alphasamples[s],
            elem_collection=elem_collection,
            elemsamples_collection=elemsamples_collection,
            min_percentile=min_percentile)
        )

    return np.array(alphasamples), np.array(lls)


def logL_alphaNP(alphaNP, elem_collection, elemsamples_collection, min_percentile):
    """
    Compute logL given:
        1. alpha value
        2. elem_collection
        3. elemsamples for each element in the collection
        4. min_percentile
    """

    for elem in elem_collection.elems:
        fitparams = elem.means_fit_params
        fitparams[-1] = alphaNP
        elem._update_fit_params(fitparams)

    # take on the two lists length
    nelemsamples = len(elemsamples_collection[0])

    loss = np.zeros(nelemsamples)

    # loop over elements in the collection
    for i, elem in enumerate(elem_collection.elems):
        # for each element, compute LL independently
        absdsamples = []
        for s in range(nelemsamples):
            elem._update_elem_params(elemsamples_collection[i][s])
            absdsamples.append(elem.absd)
        lls = get_llist(np.array(absdsamples), nelemsamples)
        delchisq = get_delchisq(lls, min_percentile)

        loss += delchisq

    return np.percentile(loss, min_percentile)


def minimise_logL_alphaNP(
        elem_collection,
        elemsamples_collection,
        maxiter,
        opt_method,
        min_percentile,
        tol=1e-12):

    if opt_method == "annealing":
        minlogL = dual_annealing(
            logL_alphaNP,
            bounds=[(-1e-4, 1e-4)],
            args=(elem_collection, elemsamples_collection, min_percentile),
            maxiter=maxiter,
        )

    elif opt_method == "differential_evolution":
        minlogL = differential_evolution(
            logL_alphaNP,
            bounds=[(-1e-4, 1e-4)],
            args=(elem_collection, elemsamples_collection, min_percentile),
            maxiter=maxiter,
        )

    else:
        minlogL = minimize(
            logL_alphaNP,
            x0=0,
            bounds=[(-1e-6, 1e-6)],
            args=(elem_collection, elemsamples_collection, min_percentile),
            method=opt_method, options={"maxiter": maxiter},
            tol=tol,
        )

    return minlogL


def get_delchisq(llist, minll=None):
    """
    Compute delta chi^2 from list of negative loglikelihoods, subtracting the
    minimum.

    """
    if minll is None:
        minll = min(llist)

    if len(llist) > 0:
        delchisqlist = 2 * (llist - minll)

        return delchisqlist

    else:
        raise ValueError(f"llist {llist} passed to get_delchisq is not a list.")


def get_delchisq_crit(nsigmas=2, dof=1):
    """
    Get chi^2 level associated to nsigmas for ``dof`` degrees of freedom.

    """

    conf_level = chi2.cdf(nsigmas**2, 1)

    return chi2.ppf(conf_level, dof)


def get_confint(alphas, delchisqs, delchisqcrit):
    """
    Get nsigmas-confidence intervals.

    Returns:
    delchisq_crit: Delta chi^2 value associated to nsigmas.
    paramlist[pos]: parameter values with Delta chi^2 values in the vicinity of
    delchisq_crit

    """
    alphas_inside = alphas[np.argwhere(delchisqs < delchisqcrit).flatten()]
    print(f"Npts in fit confidence interval: {len(alphas_inside)}")

    # Best 1% of points was used to define minll.
    # Make sure there are more than 1% of points below delchisqcrit.

    if len(alphas_inside) > 2:
        return np.array([min(alphas_inside), max(alphas_inside)])

    else:
        return np.array([np.nan, np.nan])


def determine_search_interval(
        elem_collection,
        messenger):
    # sampling `nsamples` new elements for each element in the collections

    nsearches = messenger.runparams.num_searches
    nelemsamples_search = messenger.runparams.num_elemsamples_search

    allelemsamples = []
    for elem in elem_collection.elems:
        allelemsamples.append(
            generate_element_sample(
                elem,
                nsearches * nelemsamples_search
            )
        )

    best_alpha_list = []

    print("scipy minimisation")
    for search in range(nsearches):

        if messenger.runparams.verbose is True:
            logging.info(f"Iterative search {search + 1}/{nsearches}")

        elemsamples_collection = []
        for i in range(len(elem_collection.elems)):
            elemsamples_collection.append(
                allelemsamples[i][
                    search * nelemsamples_search: (search + 1) * nelemsamples_search
                ]
            )

        res_min = minimise_logL_alphaNP(
            elem_collection=elem_collection,
            elemsamples_collection=elemsamples_collection,
            maxiter=messenger.runparams.maxiter,
            opt_method=messenger.runparams.optimization_method,
            min_percentile=messenger.runparams.min_percentile,
        )

        if res_min.success:
            best_alpha_list.append(res_min.x[0])

        print(f"res_min: {res_min}")

    best_alpha = np.median(best_alpha_list)

    sig_best_alpha = (max(best_alpha_list) - min(best_alpha_list))

    for elem in elem_collection.elems:
        elem.set_alphaNP_init(best_alpha, sig_best_alpha)

    return best_alpha, sig_best_alpha


def perform_experiments(
        elem_collection,
        messenger,
        xind=0):

    nexps = messenger.runparams.num_exp
    nelemsamples_exp = messenger.runparams.num_elemsamples_exp
    nalphasamples_exp = messenger.runparams.num_alphasamples_exp
    min_percentile = messenger.runparams.min_percentile

    # we can use one of the elements as reference, since alphaNP is shared
    allalphasamples = generate_alphaNP_sample(
        elem_collection.elems[0], nexps * nalphasamples_exp, search_mode="random"
    )

    # shuffle the sample
    np.random.shuffle(allalphasamples)

    alphas_exps, lls_exps, bestalphas_exps, delchisqs_exps = [], [], [], []

    for exp in range(nexps):
        if messenger.runparams.verbose is True:
            logging.info(f"Running experiment {exp+1}/{nexps}")

        # collect data for a single experiment
        alphasamples = allalphasamples[
            exp * nalphasamples_exp: (exp + 1) * nalphasamples_exp
        ]

        # compute alphas and LLs for this experiment
        alphas, lls = compute_ll_experiments(
            elem_collection, alphasamples, nelemsamples_exp, min_percentile,
        )

        alphas_exps.append(alphas)
        bestalphas_exps.append(alphas[np.argmin(lls)])
        lls_exps.append(lls)

        minll_1 = np.percentile(lls, min_percentile)
        delchisqlist = get_delchisq(lls, minll=minll_1)

        if messenger.runparams.verbose is True:

            from kifit.artist import plot_mc_output

            plot_mc_output(
                messenger,
                alphalist=alphas,
                delchisqlist=delchisqlist,
                minll=minll_1,
                plotname=f"exp_{exp}",
                xind=xind
            )

        delchisqs_exps.append(delchisqlist)

    nsigmas = messenger.runparams.num_sigmas

    delchisqcrit = get_delchisq_crit(nsigmas)

    confints_exps = np.array(
        [get_confint(alphas_exps[s], delchisqs_exps[s], delchisqcrit) for s in range(nexps)]
    )

    (best_alpha, sig_alpha,
        LB, sig_LB, UB, sig_UB) = \
        get_bestalphaNP_and_bounds(
            messenger,
            bestalphas_exps,
            confints_exps)

    print("LB    ", LB)
    print("sig_LB", sig_LB)
    print("UB    ", UB)
    print("sig_UB", sig_UB)

    for elem in elem_collection.elems:
        elem.set_alphaNP_init(best_alpha, sig_alpha)

    return [
        np.array(alphas_exps), np.array(delchisqs_exps),
        best_alpha, sig_alpha,
        LB, sig_LB, UB, sig_UB,
        nsigmas,
        xind
    ]


def sample_alphaNP_fit(
        elem_collection,
        messenger,
        plot_output: bool = True,
        xind: int = 0,
        verbose: bool = True):
    """
    Get a set of nsamples_search samples of elem by varying the masses and isotope
    shifts according to the means and standard deviations given in the input
    files, as well as alphaNP.

       m  ~ N(<m>,  sig[m])
       m' ~ N(<m'>, sig[m'])
       v  ~ N(<v>,  sig[v])

       alphaNP ~ N(0, sig[alphaNP_init]).

    If mphivar=True, this procedure is repeated for all X-coefficients provided
    for elem.

    """

    result_keys = [
        "alphas_exp",
        "delchisqs_exp",
        "best_alpha",
        "sig_best_alpha",
        "LB", "sig_LB",
        "UB", "sig_UB",
        "nsigmas",
        "x_ind"]

    messenger.set_fit_keys(result_keys)

    for elem in elem_collection.elems:
        elem._update_Xcoeffs(xind)

    alpha_optimizer, sig_alpha_optimizer = determine_search_interval(
        elem_collection=elem_collection,
        messenger=messenger,
    )

    fit_output = perform_experiments(
        elem_collection=elem_collection,
        messenger=messenger,
        xind=xind,
    )

    messenger.write_fit_output(xind, fit_output)

    return fit_output


def sample_alphaNP_det(
    elem,
    messenger,
    dim,
    gkp,
    xind=0
):

    alphas, sigalphas = generate_alphaNP_det_samples(
        elem=elem,
        messenger=messenger,
        dim=dim,
        gkp=gkp,
        xind=xind)

    (
        minpos_UB_alphas, maxneg_LB_alphas, minpos_alphas, maxneg_alphas
    ) = get_minpos_maxneg_alphaNP_bounds(alphas, sigalphas,
        messenger.runparams.num_sigmas)

    det_results_x = [
        elem.id,
        gkp,
        dim,
        alphas,
        sigalphas,
        minpos_UB_alphas,
        maxneg_LB_alphas,
        minpos_alphas,
        maxneg_alphas,
        messenger.runparams.num_sigmas,
        xind
    ]

    result_keys_x = [
        "elem",
        "gkp",
        "dim",
        "alphas",
        "sigalphas",
        "minpos_UB_alphas",
        "maxneg_LB_alphas",
        "minpos_alphas",
        "maxneg_alphas",
        "nsigmas",
        "x_ind"
    ]

    messenger.set_det_keys(result_keys_x)
    messenger.write_det_output(gkp, dim, xind, det_results_x)

    return det_results_x


def generate_alphaNP_det_samples(
    elem,
    messenger,
    dim,
    mphivar=False,
    gkp=True,
    xind=0
):
    """
    Get a set of nsamples samples of alphaNP by varying the masses and isotope
    shifts according to the means and standard deviations given in the input
    files:

       m  ~ N(<m>,  sig[m])
       m' ~ N(<m'>, sig[m'])
       v  ~ N(<v>,  sig[v])

    For each of these samples and for all possible combinations of the data,
    compute alphaNP using the Generalised King Plot formula with

        (nisotopepairs, ntransitions) = (dim, dim-1).

    """
    nsamples = messenger.runparams.num_det_samples

    elemparamsamples = get_paramsamples(
        elem.means_input_params, elem.stdevs_input_params, nsamples
    )

    voldatsamples = []
    vol1samples = []

    for s in range(nsamples):
        elem._update_elem_params(elemparamsamples[s])

        if gkp:
            alphaNPparts = elem.alphaNP_GKP_part(dim)
        else:
            alphaNPparts = elem.alphaNP_NMGKP_part(dim)  # this is new

        voldatsamples.append(alphaNPparts[0])
        vol1samples.append(alphaNPparts[1])
        if s == 0:
            xindlist = alphaNPparts[2]
        else:
            assert xindlist == alphaNPparts[2], (xindlist, alphaNPparts[2])

    # voldatsamples has the form [sample][alphaNP-permutation]
    # vol1samples has the form [sample][alphaNP-permutation][eps-term]

    # for each term, average over all samples.

    meanvoldat = np.average(np.array(voldatsamples), axis=0)  # [permutation]
    sigvoldat = np.std(np.array(voldatsamples), axis=0)

    meanvol1 = np.average(np.array(vol1samples), axis=0)  # [perm][eps-term]
    sigvol1 = np.std(np.array(vol1samples), axis=0)

    alphaNPs = []
    sigalphaNPs = []

    # Part with X-coeffs
    elem._update_Xcoeffs(xind)

    """ p: alphaNP-permutation index and xpinds: X-indices for sample p"""
    alphaNP_p_list = []
    sig_alphaNP_p_list = []

    for p, xpinds in enumerate(xindlist):
        meanvol1_p = np.array([elem.Xvec[xp] for xp in xpinds]) @ (meanvol1[p])
        sigvol1_p_sq = np.array([elem.Xvec[xp] ** 2 for xp in xpinds]) @ (
            sigvol1[p] ** 2
        )

        alphaNP_p_list.append(meanvoldat[p] / meanvol1_p)
        sig_alphaNP_p_list.append(
            (sigvoldat[p] / meanvol1_p) ** 2
            + (meanvoldat[p] / meanvol1_p**2) ** 2 * sigvol1_p_sq
        )

    alphaNPs = np.math.factorial(dim - 2) * np.array(alphaNP_p_list)
    sigalphaNPs = np.math.factorial(dim - 2) * np.array(sig_alphaNP_p_list)

    if gkp:
        lenp = len(list(
            product(
                combinations(elem.range_a, dim),
                combinations(elem.range_i, dim - 1))))

    else:
        lenp = len(list(
            product(
                combinations(elem.range_a, dim),
                combinations(elem.range_i, dim))))

    assert alphaNPs.shape[0] == lenp
    assert sigalphaNPs.shape[0] == lenp

    return alphaNPs, sigalphaNPs


def get_minpos_maxneg_alphaNP_bounds(alphaNPs, sigalphaNPs, nsigmas=2):
    """
    Determine smallest positive and largest negative values for the bound on
    alphaNP at the desired confidence level.

    all vectors have dimensions [x][perm]

    """
    alphaNPs = np.array(alphaNPs)  # [x][perm]
    sigalphaNPs = np.array(sigalphaNPs)  # [x][perm]

    alphaNP_UB = alphaNPs + nsigmas * sigalphaNPs
    alphaNP_LB = alphaNPs - nsigmas * sigalphaNPs

    allpos = np.where(alphaNP_UB > 0, alphaNP_UB, np.nan)
    allneg = np.where(alphaNP_LB < 0, alphaNP_LB, np.nan)

    minpos = np.nanmin(allpos)
    maxneg = np.nanmax(allneg)

    return minpos, maxneg, allpos, allneg
